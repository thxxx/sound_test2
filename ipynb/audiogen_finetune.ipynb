{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bbe7497-eb30-4101-ba66-f39b40a88907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import wandb\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from transformers import get_scheduler\n",
    "\n",
    "from config import Config\n",
    "from audiomodel import AudioProcessing\n",
    "from audiodataset import AudioDataset, TestDataset\n",
    "\n",
    "def make_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def wandb_init(cfg):\n",
    "    wandb.init(\n",
    "            # set the wandb project where this run will be logged\n",
    "            project=cfg.wandb_project_name,\n",
    "            \n",
    "            # track hyperparameters and run metadata\n",
    "            config={\n",
    "            \"learning_rate\": cfg.learning_rate,\n",
    "            \"epochs\": cfg.num_train_epochs,\n",
    "            \"batch_size\": cfg.batch_size,\n",
    "            }\n",
    "    )\n",
    "    \n",
    "def save_checkpoint(cfg, model, result, best_loss, epoch=0):\n",
    "    save_checkpoint = False\n",
    "    with open(\"{}/summary.jsonl\".format(cfg.output_dir), \"a\") as f:\n",
    "        f.write(json.dumps(result) + \"\\n\\n\")\n",
    "        \n",
    "    if result[\"train_loss\"] < best_loss:\n",
    "      best_loss = result[\"train_loss\"]\n",
    "      save_checkpoint = True\n",
    "      \n",
    "    # 모델 상태 저장\n",
    "    if save_checkpoint and cfg.checkpointing_steps == \"best\":\n",
    "        torch.save(model.state_dict(), os.path.join(cfg.output_dir, \"best.pth\"))\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(cfg.output_dir, \"last.pth\"))\n",
    "    torch.save(model.state_dict(), os.path.join(cfg.output_dir, f\"epoch_{epoch}.pth\"))\n",
    "\n",
    "    return best_loss\n",
    "\n",
    "def main():\n",
    "\n",
    "    cfg = Config()\n",
    "    \n",
    "    accelerator = Accelerator(gradient_accumulation_steps=cfg.gradient_accumulation_steps)\n",
    "    make_dir(cfg.output_dir)\n",
    "    make_dir(cfg.generated_dir)\n",
    "    wandb_init(cfg)\n",
    "    \n",
    "    #compression_model, lm = build_model(cfg)\n",
    "    model = AudioProcessing(cfg)\n",
    "    \n",
    "    audio_dataset = AudioDataset(cfg, train=True) \n",
    "    eval_dataset = AudioDataset(cfg, train=False)\n",
    "    test_dataset = TestDataset(cfg)\n",
    "\n",
    "    audio_dataloader = DataLoader(audio_dataset, batch_size=cfg.batch_size, shuffle=True, num_workers=12)\n",
    "    eval_dataloader = DataLoader(eval_dataset, batch_size=cfg.eval_batch_size, shuffle=False, num_workers=4)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "    optimizer_parameters = [param for param in model.lm.parameters() if param.requires_grad]\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        optimizer_parameters, lr=cfg.learning_rate,\n",
    "        betas=(cfg.adam_beta1, cfg.adam_beta2),\n",
    "        weight_decay=cfg.adam_weight_decay,\n",
    "        eps=cfg.adam_epsilon,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    num_update_steps_per_epoch = math.ceil(len(audio_dataloader) / cfg.gradient_accumulation_steps)\n",
    "    if cfg.max_train_steps is None:\n",
    "      cfg.max_train_steps = cfg.num_train_epochs * num_update_steps_per_epoch\n",
    "    \n",
    "    lr_scheduler = get_scheduler(\n",
    "          name=cfg.lr_scheduler_type,\n",
    "          optimizer=optimizer,\n",
    "          num_warmup_steps=cfg.num_warmup_steps * cfg.gradient_accumulation_steps,\n",
    "          num_training_steps=cfg.max_train_steps * cfg.gradient_accumulation_steps,\n",
    "      )\n",
    "\n",
    "\n",
    "    audio_dataloader, eval_dataloader, model, optimizer, lr_scheduler = accelerator.prepare(\n",
    "        audio_dataloader, eval_dataloader, model, optimizer, lr_scheduler\n",
    "    )\n",
    "\n",
    "    starting_epoch, completed_steps, best_loss = 0, 0, np.inf\n",
    "    progress_bar = tqdm(range(cfg.max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "    \n",
    "    for epoch in range(starting_epoch, cfg.num_train_epochs):\n",
    "        print(f\"-------------------EPOCH{epoch}-------------------------\" )\n",
    "        total_loss, total_val_loss = 0, 0\n",
    "        model.train()\n",
    "        for batch_idx, (wav, descriptions, lengths) in enumerate(audio_dataloader):\n",
    "          with accelerator.accumulate(model):\n",
    "              loss = model(wav, descriptions, lengths)\n",
    "              ppl =  torch.exp(loss)\n",
    "              total_loss += loss.detach().float()\n",
    "              accelerator.backward(loss)     \n",
    "              optimizer.step()\n",
    "              lr_scheduler.step()\n",
    "              optimizer.zero_grad()\n",
    "              \n",
    "          if accelerator.sync_gradients:\n",
    "              progress_bar.update(1)\n",
    "              completed_steps += 1\n",
    "            \n",
    "        model.eval()\n",
    "        for batch_idx, (wav, descriptions, lengths) in enumerate(eval_dataloader):\n",
    "              loss = model(wav, descriptions, lengths)\n",
    "              total_val_loss += loss  \n",
    "    \n",
    "        if accelerator.is_main_process:         \n",
    "            result = {}\n",
    "            result[\"epoch\"] = epoch + 1,\n",
    "            result[\"step\"] = completed_steps\n",
    "            result[\"train_loss\"] = round(total_loss.item()/len(audio_dataloader), 4)\n",
    "            result[\"valid_loss\"] = round(total_val_loss.item()/len(eval_dataloader), 4)\n",
    "            \n",
    "            wandb.log(result)\n",
    "            result_string = \"Epoch: {}, Loss Train: {}, Valid: {}\\n\".format(epoch + 1, result[\"train_loss\"], result[\"valid_loss\"])    \n",
    "            accelerator.print(result_string) \n",
    "            best_loss = save_checkpoint(cfg, model, result, best_loss, epoch)\n",
    "            \n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            for test_step, batch in enumerate(test_dataloader):\n",
    "                gen_audio = unwrapped_model.inference(batch)\n",
    "                audio_filename = f\"epoch_{epoch}_{test_step}.wav\"\n",
    "                unwrapped_model.save_audio(gen_audio, audio_filename, cfg)\n",
    "             \n",
    "    #wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b5a51e4-87db-475e-be27-5550ac28e8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on one GPU.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:wfdvdk2e) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.014 MB of 0.014 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>▁█</td></tr><tr><td>train_loss</td><td>█▁</td></tr><tr><td>valid_loss</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>92</td></tr><tr><td>train_loss</td><td>2.151</td></tr><tr><td>valid_loss</td><td>2.4137</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">driven-dragon-52</strong> at: <a href='https://wandb.ai/optimizer_ai/audiogen-finetune-init-test1/runs/wfdvdk2e' target=\"_blank\">https://wandb.ai/optimizer_ai/audiogen-finetune-init-test1/runs/wfdvdk2e</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231214_081454-wfdvdk2e/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:wfdvdk2e). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95c8a20207c844f481968d9b27df390a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112272594538, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20231214_081933-pkdbo0ih</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/optimizer_ai/audiogen-finetune-init-test1/runs/pkdbo0ih' target=\"_blank\">rural-firebrand-53</a></strong> to <a href='https://wandb.ai/optimizer_ai/audiogen-finetune-init-test1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/optimizer_ai/audiogen-finetune-init-test1' target=\"_blank\">https://wandb.ai/optimizer_ai/audiogen-finetune-init-test1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/optimizer_ai/audiogen-finetune-init-test1/runs/pkdbo0ih' target=\"_blank\">https://wandb.ai/optimizer_ai/audiogen-finetune-init-test1/runs/pkdbo0ih</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "\n",
      "  0%|          | 0/46000 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------EPOCH0-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 1/46000 [00:03<40:52:21,  3.20s/it]\u001b[A\n",
      "  0%|          | 2/46000 [00:03<20:06:14,  1.57s/it]\u001b[A\n",
      "  0%|          | 3/46000 [00:04<13:44:09,  1.08s/it]\u001b[A\n",
      "  0%|          | 4/46000 [00:04<10:43:04,  1.19it/s]\u001b[A\n",
      "  0%|          | 5/46000 [00:05<9:00:29,  1.42it/s] \u001b[A\n",
      "  0%|          | 6/46000 [00:05<8:01:04,  1.59it/s]\u001b[A\n",
      "  0%|          | 7/46000 [00:06<7:22:41,  1.73it/s]\u001b[A\n",
      "  0%|          | 8/46000 [00:06<6:57:42,  1.84it/s]\u001b[A\n",
      "  0%|          | 9/46000 [00:06<6:41:39,  1.91it/s]\u001b[A\n",
      "  0%|          | 10/46000 [00:07<6:31:49,  1.96it/s]\u001b[A\n",
      "  0%|          | 11/46000 [00:07<6:23:56,  2.00it/s]\u001b[A\n",
      "  0%|          | 12/46000 [00:08<6:19:03,  2.02it/s]\u001b[A\n",
      "  0%|          | 13/46000 [00:08<6:16:05,  2.04it/s]\u001b[A\n",
      "  0%|          | 14/46000 [00:09<6:13:20,  2.05it/s]\u001b[A\n",
      "  0%|          | 15/46000 [00:09<6:12:47,  2.06it/s]\u001b[A\n",
      "  0%|          | 16/46000 [00:10<6:09:37,  2.07it/s]\u001b[A\n",
      "  0%|          | 17/46000 [00:10<6:10:40,  2.07it/s]\u001b[A\n",
      "  0%|          | 18/46000 [00:11<6:09:34,  2.07it/s]\u001b[A\n",
      "  0%|          | 19/46000 [00:11<6:08:36,  2.08it/s]\u001b[A\n",
      "  0%|          | 20/46000 [00:12<6:07:15,  2.09it/s]\u001b[A\n",
      "  0%|          | 21/46000 [00:12<6:07:10,  2.09it/s]\u001b[A\n",
      "  0%|          | 22/46000 [00:13<6:05:50,  2.09it/s]\u001b[A\n",
      "  0%|          | 23/46000 [00:13<6:06:08,  2.09it/s]\u001b[A\n",
      "  0%|          | 24/46000 [00:14<6:07:11,  2.09it/s]\u001b[A\n",
      "  0%|          | 25/46000 [00:14<6:06:24,  2.09it/s]\u001b[A\n",
      "  0%|          | 26/46000 [00:15<6:06:42,  2.09it/s]\u001b[A\n",
      "  0%|          | 27/46000 [00:15<6:05:53,  2.09it/s]\u001b[A\n",
      "  0%|          | 28/46000 [00:16<6:06:08,  2.09it/s]\u001b[A\n",
      "  0%|          | 29/46000 [00:16<6:07:14,  2.09it/s]\u001b[A\n",
      "  0%|          | 30/46000 [00:17<6:07:23,  2.09it/s]\u001b[A\n",
      "  0%|          | 31/46000 [00:17<6:06:47,  2.09it/s]\u001b[A\n",
      "  0%|          | 32/46000 [00:17<6:07:00,  2.09it/s]\u001b[A\n",
      "  0%|          | 33/46000 [00:18<6:06:08,  2.09it/s]\u001b[A\n",
      "  0%|          | 34/46000 [00:18<6:06:19,  2.09it/s]\u001b[A\n",
      "  0%|          | 35/46000 [00:19<6:05:32,  2.10it/s]\u001b[A\n",
      "  0%|          | 36/46000 [00:19<6:06:46,  2.09it/s]\u001b[A\n",
      "  0%|          | 37/46000 [00:20<6:06:11,  2.09it/s]\u001b[A\n",
      "  0%|          | 38/46000 [00:20<6:07:21,  2.09it/s]\u001b[A\n",
      "  0%|          | 39/46000 [00:21<6:06:47,  2.09it/s]\u001b[A\n",
      "  0%|          | 40/46000 [00:21<6:04:57,  2.10it/s]\u001b[A\n",
      "  0%|          | 41/46000 [00:22<6:05:16,  2.10it/s]\u001b[A\n",
      "  0%|          | 42/46000 [00:22<6:05:21,  2.10it/s]\u001b[A\n",
      "  0%|          | 43/46000 [00:23<6:05:21,  2.10it/s]\u001b[A\n",
      "  0%|          | 44/46000 [00:23<6:04:09,  2.10it/s]\u001b[A\n",
      "  0%|          | 45/46000 [00:24<6:06:02,  2.09it/s]\u001b[A\n",
      "  0%|          | 46/46000 [00:24<6:10:26,  2.07it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss Train: 2.457, Valid: 2.4067\n",
      "\n",
      "-------------------EPOCH1-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 47/46000 [01:00<140:45:57, 11.03s/it]\u001b[A\n",
      "  0%|          | 48/46000 [01:00<100:20:00,  7.86s/it]\u001b[A\n",
      "  0%|          | 49/46000 [01:01<72:03:39,  5.65s/it] \u001b[A\n",
      "  0%|          | 50/46000 [01:01<52:17:27,  4.10s/it]\u001b[A\n",
      "  0%|          | 51/46000 [01:02<38:27:35,  3.01s/it]\u001b[A\n",
      "  0%|          | 52/46000 [01:02<28:46:16,  2.25s/it]\u001b[A\n",
      "  0%|          | 53/46000 [01:03<21:59:09,  1.72s/it]\u001b[A\n",
      "  0%|          | 54/46000 [01:03<17:13:31,  1.35s/it]\u001b[A\n",
      "  0%|          | 55/46000 [01:04<13:53:30,  1.09s/it]\u001b[A\n",
      "  0%|          | 56/46000 [01:04<11:34:12,  1.10it/s]\u001b[A\n",
      "  0%|          | 57/46000 [01:05<9:56:37,  1.28it/s] \u001b[A\n",
      "  0%|          | 58/46000 [01:05<8:49:51,  1.45it/s]\u001b[A\n",
      "  0%|          | 59/46000 [01:06<8:01:54,  1.59it/s]\u001b[A\n",
      "  0%|          | 60/46000 [01:06<7:27:37,  1.71it/s]\u001b[A\n",
      "  0%|          | 61/46000 [01:07<7:03:48,  1.81it/s]\u001b[A\n",
      "  0%|          | 62/46000 [01:07<6:47:33,  1.88it/s]\u001b[A\n",
      "  0%|          | 63/46000 [01:08<6:35:16,  1.94it/s]\u001b[A\n",
      "  0%|          | 64/46000 [01:08<6:28:24,  1.97it/s]\u001b[A\n",
      "  0%|          | 65/46000 [01:08<6:22:05,  2.00it/s]\u001b[A\n",
      "  0%|          | 66/46000 [01:09<6:17:55,  2.03it/s]\u001b[A\n",
      "  0%|          | 67/46000 [01:09<6:13:49,  2.05it/s]\u001b[A\n",
      "  0%|          | 68/46000 [01:10<6:14:20,  2.04it/s]\u001b[A\n",
      "  0%|          | 69/46000 [01:10<6:11:11,  2.06it/s]\u001b[A\n",
      "  0%|          | 70/46000 [01:11<6:10:57,  2.06it/s]\u001b[A\n",
      "  0%|          | 71/46000 [01:11<6:09:44,  2.07it/s]\u001b[A\n",
      "  0%|          | 72/46000 [01:12<6:10:03,  2.07it/s]\u001b[A\n",
      "  0%|          | 73/46000 [01:12<6:08:00,  2.08it/s]\u001b[A\n",
      "  0%|          | 74/46000 [01:13<6:06:44,  2.09it/s]\u001b[A\n",
      "  0%|          | 75/46000 [01:13<6:07:42,  2.08it/s]\u001b[A\n",
      "  0%|          | 76/46000 [01:14<6:08:14,  2.08it/s]\u001b[A\n",
      "  0%|          | 77/46000 [01:14<6:08:25,  2.08it/s]\u001b[A\n",
      "  0%|          | 78/46000 [01:15<6:07:38,  2.08it/s]\u001b[A\n",
      "  0%|          | 79/46000 [01:15<6:08:26,  2.08it/s]\u001b[A\n",
      "  0%|          | 80/46000 [01:16<6:08:47,  2.08it/s]\u001b[A\n",
      "  0%|          | 81/46000 [01:16<6:08:38,  2.08it/s]\u001b[A\n",
      "  0%|          | 82/46000 [01:17<6:08:01,  2.08it/s]\u001b[A\n",
      "  0%|          | 83/46000 [01:17<6:06:59,  2.09it/s]\u001b[A\n",
      "  0%|          | 84/46000 [01:18<6:07:29,  2.08it/s]\u001b[A\n",
      "  0%|          | 85/46000 [01:18<6:08:59,  2.07it/s]\u001b[A\n",
      "  0%|          | 86/46000 [01:19<6:08:12,  2.08it/s]\u001b[A\n",
      "  0%|          | 87/46000 [01:19<6:08:41,  2.08it/s]\u001b[A\n",
      "  0%|          | 88/46000 [01:20<6:06:58,  2.09it/s]\u001b[A\n",
      "  0%|          | 89/46000 [01:20<6:07:56,  2.08it/s]\u001b[A\n",
      "  0%|          | 90/46000 [01:21<6:08:33,  2.08it/s]\u001b[A\n",
      "  0%|          | 91/46000 [01:21<6:09:23,  2.07it/s]\u001b[A\n",
      "  0%|          | 92/46000 [01:21<6:00:00,  2.13it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss Train: 2.151, Valid: 2.4137\n",
      "\n",
      "-------------------EPOCH2-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 93/46000 [01:57<139:14:47, 10.92s/it]\u001b[A\n",
      "  0%|          | 94/46000 [01:57<99:18:36,  7.79s/it] \u001b[A\n",
      "  0%|          | 95/46000 [01:58<71:21:33,  5.60s/it]\u001b[A\n",
      "  0%|          | 96/46000 [01:58<51:45:49,  4.06s/it]\u001b[A\n",
      "  0%|          | 97/46000 [01:59<38:04:44,  2.99s/it]\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#main()\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m notebook_launcher\n\u001b[0;32m----> 3\u001b[0m \u001b[43mnotebook_launcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/launchers.py:207\u001b[0m, in \u001b[0;36mnotebook_launcher\u001b[0;34m(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLaunching training on CPU.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 207\u001b[0m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 112\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (wav, descriptions, lengths) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(audio_dataloader):\n\u001b[1;32m    111\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m accelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m--> 112\u001b[0m       loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwav\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescriptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m       ppl \u001b[38;5;241m=\u001b[39m  torch\u001b[38;5;241m.\u001b[39mexp(loss)\n\u001b[1;32m    114\u001b[0m       total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/audiomodel.py:32\u001b[0m, in \u001b[0;36mAudioProcessing.forward\u001b[0;34m(self, wav, descriptions, lengths)\u001b[0m\n\u001b[1;32m     26\u001b[0m audio_tokens, padding_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_process_audio_tokenizer(audio_tokens, audio_lengths\u001b[38;5;241m=\u001b[39mlengths)\n\u001b[1;32m     28\u001b[0m attributes \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     29\u001b[0m     ConditioningAttributes(text\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m: description})\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m description \u001b[38;5;129;01min\u001b[39;00m descriptions]\n\u001b[0;32m---> 32\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconditions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcondition_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     33\u001b[0m logits \u001b[38;5;241m=\u001b[39m model_output\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     35\u001b[0m mask \u001b[38;5;241m=\u001b[39m padding_mask \u001b[38;5;241m&\u001b[39m model_output\u001b[38;5;241m.\u001b[39mmask\n",
      "File \u001b[0;32m/workspace/audiocraft/models/lm.py:297\u001b[0m, in \u001b[0;36mLMModel.compute_predictions\u001b[0;34m(self, codes, conditions, condition_tensors)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# apply model on pattern sequence\u001b[39;00m\n\u001b[1;32m    296\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fsdp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fsdp\n\u001b[0;32m--> 297\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_codes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconditions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcondition_tensors\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [B, K, S, card]\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;66;03m# map back the logits on pattern sequence to logits on original codes: [B, K, S, card] -> [B, K, T, card]\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# and provide the corresponding mask over invalid positions of tokens\u001b[39;00m\n\u001b[1;32m    300\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# [B, card, K, S]\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/audiocraft/models/lm.py:253\u001b[0m, in \u001b[0;36mLMModel.forward\u001b[0;34m(self, sequence, conditions, condition_tensors)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conditions, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt pass both conditions and condition_tensors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    251\u001b[0m input_, cross_attention_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuser(input_, condition_tensors)\n\u001b[0;32m--> 253\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcross_attention_src\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_norm:\n\u001b[1;32m    255\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_norm(out)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/audiocraft/modules/transformer.py:698\u001b[0m, in \u001b[0;36mStreamingTransformer.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    695\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_scale \u001b[38;5;241m*\u001b[39m pos_emb\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 698\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_streaming:\n\u001b[1;32m    701\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_streaming_state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moffsets\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m offsets \u001b[38;5;241m+\u001b[39m T\n",
      "File \u001b[0;32m/workspace/audiocraft/modules/transformer.py:655\u001b[0m, in \u001b[0;36mStreamingTransformer._apply_layer\u001b[0;34m(self, layer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    653\u001b[0m method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpointing\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 655\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_checkpoint(layer, \u001b[38;5;241m*\u001b[39margs, use_reentrant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/audiocraft/modules/transformer.py:550\u001b[0m, in \u001b[0;36mStreamingTransformerLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, cross_attention_src)\u001b[0m\n\u001b[1;32m    547\u001b[0m x \u001b[38;5;241m=\u001b[39m src\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_first:\n\u001b[1;32m    549\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_scale_1(\n\u001b[0;32m--> 550\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cross_attention_src \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    552\u001b[0m         x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_scale_cross(\n\u001b[1;32m    553\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cross_attention_block(\n\u001b[1;32m    554\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_cross(x), cross_attention_src))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:715\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sa_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor,\n\u001b[1;32m    714\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 715\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/audiocraft/modules/transformer.py:372\u001b[0m, in \u001b[0;36mStreamingMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m    370\u001b[0m         bound_layout \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb t p h d\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m     packed \u001b[38;5;241m=\u001b[39m rearrange(projected, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb t (p h d) -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbound_layout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, h\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads)\n\u001b[0;32m--> 372\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpacked\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    374\u001b[0m     embed_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/xformers/ops/unbind.py:117\u001b[0m, in \u001b[0;36munbind\u001b[0;34m(x, dim)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munbind\u001b[39m(x: torch\u001b[38;5;241m.\u001b[39mTensor, dim: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[1;32m    112\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    Does exactly the same as :attr:`torch.unbind` for the forward.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m    In backward, avoids a :attr:`torch.cat` if the gradients\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m    are already multiple views of the same storage\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_Unbind\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:536\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 536\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_are_functorch_transforms_active\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    537\u001b[0m         \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m         args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[1;32m    539\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#main()\n",
    "from accelerate import notebook_launcher\n",
    "notebook_launcher(main, num_processes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f65cfa-85aa-4bc7-b7a7-770a50dd7df7",
   "metadata": {},
   "source": [
    "### Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "325e8d99-5bcc-4503-b82c-00fb9c1f1204",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "import tqdm\n",
    "import json\n",
    "import typing as tp\n",
    "import pandas as pd\n",
    "import glob2\n",
    "import math\n",
    "import omegaconf\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.sample_rate = 16000\n",
    "        self.is_training = True\n",
    "        self.duration = 1\n",
    "        self.total_updates = 10000\n",
    "        self.eval_steps = 4\n",
    "        self.device = 'cuda'  # 'cuda' 또는 'cpu'\n",
    "        self.batch_size = 24\n",
    "        self.eval_batch_size = 4\n",
    "        self.train_data_path = \"/workspace/train_dataset.csv\"\n",
    "        self.eval_data_path = \"/workspace/eval_dataset.csv\"\n",
    "        self.output_dir = \"./output_dir\"\n",
    "        self.checkpointing_steps = \"best\"\n",
    "        self.save_every = 10\n",
    "        self.with_tracking = False\n",
    "        self.text_encoder_name = None  # 나중에 설정\n",
    "        self.snr_gamma = 5.0\n",
    "        self.freeze_text_encoder = True\n",
    "        self.uncondition = False\n",
    "        self.learning_rate = 3e-5\n",
    "        self.adam_beta1 = 0.9\n",
    "        self.adam_beta2 = 0.999\n",
    "        self.adam_weight_decay = 1e-2\n",
    "        self.adam_epsilon = 1e-08\n",
    "        self.gradient_accumulation_steps = 1\n",
    "        self.num_train_epochs = 1000\n",
    "        self.num_warmup_steps = 0\n",
    "        self.max_train_steps = None\n",
    "        self.lr_scheduler_type = \"linear\"\n",
    "        self.resume_from_checkpoint = None #\"/workspace/output_dir_batch48/last/\" #None\n",
    "        self.wandb_project_name = \"audiogen-finetune-init-test1\"\n",
    "        self.wandb_id = None #\"earnest-pond-52\"\n",
    "        self.resume_epoch = 0 #127\n",
    "        self.dtype = \"float32\"\n",
    "        \n",
    "        self.update_audiocraft_config()\n",
    "        \n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            # 기존 속성에 값 할당하거나 새 속성 생성\n",
    "            if not hasattr(self, key):\n",
    "                print(key)\n",
    "            setattr(self, key, value)\n",
    "            \n",
    "    def update_audiocraft_config(self):\n",
    "        self.solver = None\n",
    "        self.fsdp = None\n",
    "        self.profiler = None\n",
    "        self.deadlock = None\n",
    "        self.dataset = None\n",
    "        self.checkpoint = None\n",
    "        self.generate = None\n",
    "        self.evaluate = None\n",
    "        self.optim = None\n",
    "        self.schedule = None\n",
    "        self.default = None\n",
    "        self.defaults = None\n",
    "        self.autocast = None\n",
    "        self.autocast_dtype = None\n",
    "\n",
    "        self.compression_model_checkpoint = None\n",
    "        self.channels = None\n",
    "        self.logging = None\n",
    "        self.lm_model = None\n",
    "        self.codebooks_pattern = None\n",
    "        self.transformer_lm = None\n",
    "        self.classifier_free_guidance = None\n",
    "        self.attribute_dropout = None\n",
    "        self.fuser = None\n",
    "        self.conditioners = None\n",
    "        self.datasource = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "463fbdc2-a1ab-405d-b417-a9dfe15418fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_paths, device, target_sample_rate=44100, duration=3):\n",
    "        import pandas as pd\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio_files_list (list): List of paths to audio files.\n",
    "            target_sample_rate (int): The sample rate to which audio should be resampled.\n",
    "            frame_length (int): The frame length for slicing or padding audio.\n",
    "        \"\"\"\n",
    "        self.audio_paths = audio_paths\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.duration = duration\n",
    "        self.device = device\n",
    "\n",
    "        self.df = pd.read_csv(self.audio_paths)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        data = self.df.iloc[idx] #self.audio_files_list[idx]\n",
    "        audio_path = data['sliced_audio_path']\n",
    "        description = data['description']\n",
    "\n",
    "        # Load audio signal file\n",
    "        from audiotools import AudioSignal\n",
    "        wav = AudioSignal(audio_path)\n",
    "        length = wav.signal_length\n",
    "\n",
    "        # Encode audio signal as one long file\n",
    "        wav.to_mono()\n",
    "        wav.resample(self.target_sample_rate)\n",
    "\n",
    "        if wav.duration < self.duration:\n",
    "          pad_len = int(self.duration * self.target_sample_rate) - wav.signal_length\n",
    "          wav.zero_pad(0, pad_len)\n",
    "        elif wav.duration > self.duration:\n",
    "          wav.truncate_samples(self.duration * self.target_sample_rate)\n",
    "\n",
    "\n",
    "        return wav.audio_data.squeeze(1), description, length\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, prompts):\n",
    "\n",
    "        self.prompts = prompts\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.prompts[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c36c688-9e32-408e-b60f-01811aa9fe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiocraft.solvers import base, builders\n",
    "from audiocraft.solvers.compression import CompressionSolver\n",
    "from audiocraft import metrics as eval_metrics\n",
    "from audiocraft import models\n",
    "from audiocraft.data.audio_utils import normalize_audio\n",
    "from audiocraft.modules.conditioners import JointEmbedCondition, SegmentWithAttributes, WavCondition, ConditioningAttributes\n",
    "from audiocraft.utils.utils import get_dataset_from_loader, is_jsonable, warn_once\n",
    "from audiocraft.models.loaders import load_compression_model, load_lm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2516970a-a1ee-4324-81cc-17167ede6e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioProcessing(nn.Module):\n",
    "    \n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()  # 부모 클래스 초기화 호출\n",
    "        self.cfg = cfg\n",
    "        self.compression_model, self.lm  = self.build_model(self.cfg)\n",
    "        self.to_float32()\n",
    "        self.freeze_layers()\n",
    "\n",
    "    def forward(self, wav, descriptions, lengths):\n",
    "        from audiocraft.modules.conditioners import JointEmbedCondition, SegmentWithAttributes, WavCondition, ConditioningAttributes\n",
    "        audio_tokens = self.process_audio_tokenizer(wav.to(self.cfg.device))\n",
    "        audio_tokens, padding_mask = self.post_process_audio_tokenizer(audio_tokens, audio_lengths=lengths)\n",
    "        \n",
    "        attributes = [\n",
    "            ConditioningAttributes(text={'description': description})\n",
    "            for description in descriptions]\n",
    "    \n",
    "        model_output = self.lm.compute_predictions(audio_tokens, conditions=attributes, condition_tensors=None)  # type: ignore\n",
    "        logits = model_output.logits\n",
    "    \n",
    "        mask = padding_mask & model_output.mask\n",
    "        ce, ce_per_codebook = self.compute_cross_entropy(logits, audio_tokens, mask)\n",
    "        \n",
    "        return ce\n",
    "\n",
    "    def build_model(self, cfg):\n",
    "        from audiocraft.models.loaders import load_compression_model, load_lm_model\n",
    "        \"\"\"Instantiate models and optimizer.\"\"\"\n",
    "        \n",
    "        compression_model = load_compression_model('facebook/audiogen-medium', device=cfg.device)\n",
    "        lm = load_lm_model('facebook/audiogen-medium', device=cfg.device)\n",
    "    \n",
    "        return compression_model, lm\n",
    "\n",
    "    def process_audio_tokenizer(self, wav):\n",
    "        with torch.no_grad():\n",
    "            audio_tokens, scale = self.compression_model.encode(wav)\n",
    "        return audio_tokens\n",
    "\n",
    "    def post_process_audio_tokenizer(self, audio_tokens, audio_lengths=None):\n",
    "        padding_mask = torch.ones_like(audio_tokens, dtype=torch.bool, device=audio_tokens.device)\n",
    "\n",
    "        audio_tokens = audio_tokens.clone()\n",
    "        padding_mask = padding_mask.clone()\n",
    "        token_sample_rate = self.compression_model.frame_rate\n",
    "        B, K, T_s = audio_tokens.shape\n",
    "        for i in range(B):\n",
    "            valid_tokens = math.floor(audio_lengths[i] / self.cfg.sample_rate * token_sample_rate)\n",
    "            audio_tokens[i, :, valid_tokens:] = self.lm.special_token_id\n",
    "            padding_mask[i, :, valid_tokens:] = 0\n",
    "\n",
    "        return audio_tokens, padding_mask\n",
    "\n",
    "    def compute_cross_entropy(self, logits: torch.Tensor, targets: torch.Tensor, mask: torch.Tensor) -> tp.Tuple[torch.Tensor, tp.List[torch.Tensor]]:\n",
    "\n",
    "        B, K, T = targets.shape\n",
    "        assert logits.shape[:-1] == targets.shape\n",
    "        assert mask.shape == targets.shape\n",
    "        ce = torch.zeros([], device=targets.device)\n",
    "        ce_per_codebook: tp.List[torch.Tensor] = []\n",
    "        for k in range(K):\n",
    "            logits_k = logits[:, k, ...].contiguous().view(-1, logits.size(-1))  # [B x T, card]\n",
    "            targets_k = targets[:, k, ...].contiguous().view(-1)  # [B x T]\n",
    "            mask_k = mask[:, k, ...].contiguous().view(-1)  # [B x T]\n",
    "            ce_targets = targets_k[mask_k]\n",
    "            ce_logits = logits_k[mask_k]\n",
    "            q_ce = F.cross_entropy(ce_logits, ce_targets)\n",
    "            ce += q_ce\n",
    "            ce_per_codebook.append(q_ce.detach())\n",
    "        # average cross entropy across codebooks\n",
    "        ce = ce / K\n",
    "        return ce, ce_per_codebook\n",
    "\n",
    "    def audio_generate(self, condition_tensors, gen_duration=5):\n",
    "        with torch.no_grad():\n",
    "            total_gen_len = math.ceil(gen_duration * self.compression_model.frame_rate)\n",
    "            gen_tokens = self.lm.generate(\n",
    "                None, condition_tensors, max_gen_len=total_gen_len,\n",
    "                num_samples=1)\n",
    "            gen_audio = self.compression_model.decode(gen_tokens, None)\n",
    "\n",
    "        return gen_tokens, gen_audio\n",
    "\n",
    "    def inference(self, descriptions):\n",
    "        #with torch.no_grad():\n",
    "        from audiocraft.modules.conditioners import JointEmbedCondition, SegmentWithAttributes, WavCondition, ConditioningAttributes\n",
    "        attributes = [\n",
    "        ConditioningAttributes(text={'description': description})\n",
    "        for description in descriptions]\n",
    "        _, gen_audio = self.audio_generate(attributes, gen_duration=self.cfg.duration)\n",
    "        \n",
    "        return gen_audio\n",
    "\n",
    "    def to_float32(self):\n",
    "        # 모든 가중치를 FP32로 변환\n",
    "        for param in self.lm.parameters():\n",
    "            param.data = param.data.to(dtype=torch.float32)\n",
    "\n",
    "    def freeze_layers(self, train_layers=12):\n",
    "        for param in self.lm.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        if train_layers > 0 :\n",
    "            num_layers = len(self.lm.transformer.layers)\n",
    "            \n",
    "            for i in range(num_layers - train_layers, num_layers):\n",
    "                for param in self.lm.transformer.layers[i].parameters():\n",
    "                    param.requires_grad = True\n",
    "                    \n",
    "            for name, param in self.lm.named_parameters():\n",
    "                if 'out_norm' in name or 'linears' in name:\n",
    "                    param.requires_grad = True\n",
    "                \n",
    "            \n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "102033ad-06dd-418f-8a63-5dcb73587ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract discrete codes from EnCodec\n",
    "def process_audio_tokenizer(wav):\n",
    "  with torch.no_grad():\n",
    "      audio_tokens, scale = compression_model.encode(wav)\n",
    "  return audio_tokens\n",
    "\n",
    "def post_process_audio_tokenizer(audio_tokens, audio_lengths=None, cfg=None):\n",
    "  padding_mask = torch.ones_like(audio_tokens, dtype=torch.bool, device=audio_tokens.device)\n",
    "  # replace encodec tokens from padded audio with special_token_id\n",
    "\n",
    "  audio_tokens = audio_tokens.clone()\n",
    "  padding_mask = padding_mask.clone()\n",
    "  token_sample_rate = compression_model.frame_rate\n",
    "  B, K, T_s = audio_tokens.shape\n",
    "  for i in range(B):\n",
    "      # take the last token generated from actual audio frames (non-padded audio)\n",
    "      #math.floor(float(n_frames[i]) / sr[i] * token_sample_rate)\n",
    "      valid_tokens = math.floor(audio_lengths[i] / cfg.sample_rate * token_sample_rate)\n",
    "      audio_tokens[i, :, valid_tokens:] = lm.special_token_id\n",
    "      padding_mask[i, :, valid_tokens:] = 0\n",
    "\n",
    "  return audio_tokens, padding_mask\n",
    "\n",
    "def _compute_cross_entropy(\n",
    "      logits: torch.Tensor, targets: torch.Tensor, mask: torch.Tensor\n",
    "    ) -> tp.Tuple[torch.Tensor, tp.List[torch.Tensor]]:\n",
    "        \"\"\"Compute cross entropy between multi-codebook targets and model's logits.\n",
    "        The cross entropy is computed per codebook to provide codebook-level cross entropy.\n",
    "        Valid timesteps for each of the codebook are pulled from the mask, where invalid\n",
    "        timesteps are set to 0.\n",
    "\n",
    "        Args:\n",
    "            logits (torch.Tensor): Model's logits of shape [B, K, T, card].\n",
    "            targets (torch.Tensor): Target codes, of shape [B, K, T].\n",
    "            mask (torch.Tensor): Mask for valid target codes, of shape [B, K, T].\n",
    "        Returns:\n",
    "            ce (torch.Tensor): Cross entropy averaged over the codebooks\n",
    "            ce_per_codebook (list of torch.Tensor): Cross entropy per codebook (detached).\n",
    "        \"\"\"\n",
    "        B, K, T = targets.shape\n",
    "        assert logits.shape[:-1] == targets.shape\n",
    "        assert mask.shape == targets.shape\n",
    "        ce = torch.zeros([], device=targets.device)\n",
    "        ce_per_codebook: tp.List[torch.Tensor] = []\n",
    "        for k in range(K):\n",
    "            logits_k = logits[:, k, ...].contiguous().view(-1, logits.size(-1))  # [B x T, card]\n",
    "            targets_k = targets[:, k, ...].contiguous().view(-1)  # [B x T]\n",
    "            mask_k = mask[:, k, ...].contiguous().view(-1)  # [B x T]\n",
    "            ce_targets = targets_k[mask_k]\n",
    "            ce_logits = logits_k[mask_k]\n",
    "            q_ce = F.cross_entropy(ce_logits, ce_targets)\n",
    "            ce += q_ce\n",
    "            ce_per_codebook.append(q_ce.detach())\n",
    "        # average cross entropy across codebooks\n",
    "        ce = ce / K\n",
    "        return ce, ce_per_codebook\n",
    "\n",
    "def audio_generate(condition_tensors, gen_duration=5):\n",
    "    with torch.no_grad():\n",
    "      total_gen_len = math.ceil(gen_duration * compression_model.frame_rate)\n",
    "      gen_tokens = lm.generate(\n",
    "          None, condition_tensors, max_gen_len=total_gen_len,\n",
    "          num_samples=1)\n",
    "      gen_audio = compression_model.decode(gen_tokens, None)\n",
    "\n",
    "    return gen_tokens, gen_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b264f61-dfaa-40a1-9510-328b9f9049c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav, text, length = next(iter(audio_dataloader))\n",
    "audio_tokens = process_audio_tokenizer(wav.to(cfg.device))\n",
    "print(\"Wav shape: \", wav.shape)\n",
    "print(\"Token shape: \", audio_tokens.shape)\n",
    "post_process_audio_tokenizer(audio_tokens, audio_lengths=length)\n",
    "\n",
    "import torch\n",
    "\n",
    "# 가정: model이라는 이름의 PyTorch 모델이 이미 정의되어 있음\n",
    "for name, param in lm.named_parameters():\n",
    "    print(f\"Layer {name} has data type {param.dtype}\")\n",
    "    #break  # 모든 레이어를 표시하지 않고 첫 레이어에서 루프 중단\n",
    "\n",
    "def check_requires_grad(model: torch.nn.Module):\n",
    "    for name, module in model.named_children():\n",
    "        for param_name, param in module.named_parameters():\n",
    "            print(f\"{name}.{param_name}: requires_grad = {param.requires_grad}\")\n",
    "\n",
    "# DAC 모델의 인스턴스를 생성한 후에 아래와 같이 사용할 수 있습니다:\n",
    "# dac_instance = DAC(...)\n",
    "check_requires_grad(lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5283af14-dd57-406b-8834-58fb21dcc208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2b4a7e-1ca5-4749-84a3-c5543e669d2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
